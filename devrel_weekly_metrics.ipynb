{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ccb0da",
   "metadata": {},
   "source": [
    "# Imports and Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74fda44",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72905311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 6\n",
    "\n",
    "import datetime\n",
    "import dateutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeb4a39",
   "metadata": {},
   "source": [
    "## functions and variables needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478aeabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unauthenticated requests have lower rate limit - this is why we authenticate\n",
    "github_user_name = \"UPDATE_WITH_YOUR_USERNAME\"\n",
    "#TODO: set to your GitHub username\n",
    "github_oauth_token = \"UPDATE_WITH_YOUR_TOKEN\"\n",
    "#TODO: set to your personal GitHib OAuth token  # get one here: https://github.com/settings/tokens\n",
    "\n",
    "min_fetch_created_datetime = datetime.datetime.strftime(datetime.datetime.now() + datetime.timedelta(days=-180), \"%Y-%m-%d\") + \"T00:00:00Z\"\n",
    "print(min_fetch_created_datetime)\n",
    "\n",
    "min_report_created_datetime = datetime.datetime.now() + datetime.timedelta(days=-90)\n",
    "min_report_created_datetime_str = datetime.datetime.strftime(min_report_created_datetime, \"%Y-%m-%d\") + \"T00:00:00Z\"\n",
    "print(min_report_created_datetime_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2b1b1",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# issue functions\n",
    "def send_request_to_github_api(url):\n",
    "    # We use basic auth with a personal OAuth token: https://developer.github.com/v3/auth/#basic-authentication\n",
    "    \n",
    "    headers = {'Accept': 'application/vnd.github.v3+json'\n",
    "               }\n",
    "    \n",
    "    response = requests.get(url,\n",
    "                            auth=(github_user_name, github_oauth_token),\n",
    "                            headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\"HTTP Status: {0:s}, Reason: {1:s}\".format(str(response.status_code), response.reason))\n",
    "    \n",
    "    return response\n",
    "\n",
    "def sunday_of_the_week_date(dt):\n",
    "#     sunday_dt = (dt + datetime.timedelta(\n",
    "#                 days=(6 - dt.weekday()), weeks=-1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    if not dt:\n",
    "        return None\n",
    "    elif dt.weekday() == 6:\n",
    "        sunday_dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    else:\n",
    "        sunday_dt = (dt + datetime.timedelta(\n",
    "                days=(6 - dt.weekday()), weeks=-1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "    return sunday_dt\n",
    "\n",
    "def sunday_of_the_week_date_str(dt):\n",
    "    if not dt:\n",
    "        return None\n",
    "    elif dt.weekday() == 6:\n",
    "        sunday_dt = dt.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    else:\n",
    "        sunday_dt = (dt + datetime.timedelta(\n",
    "                days=(6 - dt.weekday()), weeks=-1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    return datetime.datetime.strftime(sunday_dt, \"%Y-%m-%d\")\n",
    "\n",
    "def get_all_info_for_issue_row(issue_info):\n",
    "    issue_close_datetime = get_issue_close_datetime(issue_info)\n",
    "    first_core_team_issue_comment_datetime = get_first_core_team_issue_comment_datetime(issue_info)\n",
    "    \n",
    "    return {\n",
    "        \"number\": issue_info[\"number\"],\n",
    "        \"author_login\": issue_info[\"user\"][\"login\"],\n",
    "        \"created_dt\": issue_info[\"created_at\"],\n",
    "        \"is_core_engineering_queue\": has_label_with_name(issue_info, \"core-engineering-queue\"),\n",
    "        \"is_help_wanted\": has_label_with_name(issue_info, \"help_wanted\"),\n",
    "        \"is_devrel\": has_label_with_name(issue_info, \"devrel\"),\n",
    "        \"is_triage\": has_label_with_name(issue_info, \"triage\"),\n",
    "        \"first_core_team_comment_dt\": first_core_team_issue_comment_datetime,\n",
    "        \"close_dt\": issue_close_datetime,\n",
    "        \"priority_category\": get_issue_category_from_labels(issue_info)\n",
    "    }\n",
    "    \n",
    "def get_issue_close_datetime(issue_info):\n",
    "    return issue_info[\"closed_at\"]\n",
    "\n",
    "def has_label_with_name(issue_info, label_name):\n",
    "    issue_labels = issue_info.get(\"labels\")\n",
    "    if issue_labels is not None:\n",
    "        return len([issue_label for issue_label in issue_labels if issue_label[\"name\"] == label_name]) > 0\n",
    "    return False\n",
    "    \n",
    "\n",
    "def get_issue_category_from_labels(issue_info):\n",
    "    issue_labels = issue_info.get(\"labels\")\n",
    "    if issue_labels is not None:\n",
    "        for issue_label in issue_labels:\n",
    "            if issue_label[\"name\"] == 'core-engineering-queue':\n",
    "                return 'core-engineering-queue'\n",
    "            if issue_label[\"name\"] == 'help wanted':\n",
    "                return 'help wanted'\n",
    "            if issue_label[\"name\"] == 'devrel':\n",
    "                return 'devrel'\n",
    "            if issue_label[\"name\"] == 'triage':\n",
    "                return 'triage'\n",
    "            \n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def get_first_core_team_issue_comment_datetime(issue_info):\n",
    "    issue_number = issue_info[\"number\"]\n",
    "    first_core_team_issue_comment_datetime = None\n",
    "    \n",
    "    issue_comments_response = send_request_to_github_api(\"https://api.github.com/repos/great-expectations/great_expectations/issues/{0:d}/comments\".format(issue_number))\n",
    "\n",
    "    core_team_issue_comment_datetimes = sorted([comment[\"created_at\"] for comment in issue_comments_response.json() if comment[\"user\"][\"login\"] in core_team_usernames])\n",
    "    \n",
    "    if len(core_team_issue_comment_datetimes) > 0:\n",
    "        first_core_team_issue_comment_datetime = core_team_issue_comment_datetimes[0]\n",
    "\n",
    "    return first_core_team_issue_comment_datetime\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df68cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PR functions\n",
    "def get_all_info_for_pr_row(pr_info):\n",
    "    pr_merge_datetime = get_pr_merge_datetime(pr_info)\n",
    "    pr_close_datetime = get_pr_close_datetime(pr_info)\n",
    "    first_core_team_pr_comment_datetime = get_first_core_team_pr_comment_datetime(pr_info)\n",
    "    first_core_team_pr_review_datetime, first_core_team_approval_datetime = get_first_core_team_pr_review_and_approval_datetimes(pr_info)\n",
    "\n",
    "    if first_core_team_pr_comment_datetime is not None and first_core_team_pr_review_datetime is not None:\n",
    "        first_core_team_reaction_datetime = sorted([first_core_team_pr_comment_datetime, first_core_team_pr_review_datetime])[0]\n",
    "    else: \n",
    "        first_core_team_reaction_datetime = first_core_team_pr_comment_datetime or first_core_team_pr_review_datetime\n",
    "    \n",
    "    return {\n",
    "        \"number\": pr_info[\"number\"],\n",
    "        \"created_dt\": pr_info[\"created_at\"],\n",
    "        \"first_core_team_reaction_dt\": first_core_team_reaction_datetime,\n",
    "        \"first_core_team_comment_dt\": first_core_team_pr_comment_datetime,\n",
    "        \"first_core_team_review_dt\": first_core_team_pr_review_datetime,\n",
    "        \"first_core_team_approval_dt\": first_core_team_approval_datetime,\n",
    "        \"merge_dt\": pr_merge_datetime,\n",
    "        \"close_dt\": pr_close_datetime,\n",
    "    }\n",
    "    \n",
    "    \n",
    "def get_pr_close_datetime(pr_info):\n",
    "    return pr_info[\"closed_at\"]\n",
    "\n",
    "def get_pr_merge_datetime(pr_info):\n",
    "    return pr_info[\"merged_at\"]\n",
    "    \n",
    "def get_first_core_team_pr_comment_datetime(pr_info):\n",
    "    pr_number = pr_info[\"number\"]\n",
    "    first_core_team_pr_comment_datetime = None\n",
    "    \n",
    "    pr_comments_response = send_request_to_github_api(\"https://api.github.com/repos/great-expectations/great_expectations/issues/{0:d}/comments\".format(pr_number))\n",
    "\n",
    "    core_team_pr_comment_datetimes = sorted([comment[\"created_at\"] for comment in pr_comments_response.json() if comment[\"user\"][\"login\"] in core_team_usernames])\n",
    "    \n",
    "    if len(core_team_pr_comment_datetimes) > 0:\n",
    "        first_core_team_pr_comment_datetime = core_team_pr_comment_datetimes[0]\n",
    "\n",
    "    return first_core_team_pr_comment_datetime\n",
    "    \n",
    "def get_first_core_team_pr_review_and_approval_datetimes(pr_info):\n",
    "    pr_number = pr_info[\"number\"]\n",
    "    first_core_team_pr_review_datetime = None\n",
    "    first_core_team_pr_approval_datetime = None\n",
    "    \n",
    "    pr_reviews_response = send_request_to_github_api(\"https://api.github.com/repos/great-expectations/great_expectations/pulls/{0:d}/reviews\".format(pr_number))\n",
    "    \n",
    "    core_team_pr_review_datetimes = sorted([review[\"submitted_at\"] for review in pr_reviews_response.json() if review[\"user\"][\"login\"] in core_team_usernames])\n",
    "    if len(core_team_pr_review_datetimes) > 0:\n",
    "        first_core_team_pr_review_datetime = core_team_pr_review_datetimes[0]\n",
    "\n",
    "    core_team_pr_approval_datetimes = sorted([review[\"submitted_at\"] for review in pr_reviews_response.json() if review[\"user\"][\"login\"] in core_team_usernames and review[\"state\"] == \"APPROVED\"])\n",
    "    if len(core_team_pr_approval_datetimes) > 0:\n",
    "        first_core_team_pr_approval_datetime = core_team_pr_approval_datetimes[0]\n",
    "\n",
    "    return (first_core_team_pr_review_datetime, first_core_team_pr_approval_datetime)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3776dc63",
   "metadata": {},
   "source": [
    "## core team list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4f3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_team_usernames = { # these are former team members to filter out. We will additionally add current members of the organization\n",
    "    'spbail-ge',\n",
    "    'spbail',\n",
    "    'Aylr',\n",
    "    'dependabot[bot]',\n",
    "    'gilpasternak35',\n",
    "    'snyk-bot',\n",
    "    'dependabot'\n",
    "}\n",
    "\n",
    "url_sc = \"https://api.github.com/orgs/superconductive/members\"\n",
    "url_ge = \"https://api.github.com/orgs/great-expectations/members\"\n",
    "\n",
    "for member_organization_url in [url_sc, url_ge]:\n",
    "    members = send_request_to_github_api(member_organization_url)\n",
    "    core_team_usernames.update({i.get(\"login\") for i in members.json()})\n",
    "\n",
    "core_team_usernames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70464d4f",
   "metadata": {},
   "source": [
    "# Contributor Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff26ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.github.com/repos/great-expectations/great_expectations/stats/contributors\")\n",
    "response_json = response.json()\n",
    "response_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_list = []\n",
    "\n",
    "for contrib in response_json:\n",
    "    contributor_list.append(contrib[\"author\"])\n",
    "\n",
    "contributor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_df = pd.DataFrame(contributor_list)\n",
    "contributor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_week_list = []\n",
    "all_weeks = set()\n",
    "\n",
    "for contrib in response_json:\n",
    "    \n",
    "    first_contrib = True\n",
    "    for week in contrib[\"weeks\"]:\n",
    "        week[\"login\"] = contrib[\"author\"][\"login\"]\n",
    "        week[\"contributors\"] = 1\n",
    "    \n",
    "        if week[\"c\"] > 0:\n",
    "            week[\"first_contrib\"] = first_contrib\n",
    "            contributor_week_list.append(week)\n",
    "            \n",
    "            #This method of detecting first contributions will only work if week timestamps in \"w\" are sorted in ascending order\n",
    "            #This appears to be true, but is untested.\n",
    "            first_contrib = False\n",
    "            \n",
    "        all_weeks.add(week[\"w\"])\n",
    "        \n",
    "contributor_week_df = pd.DataFrame(contributor_week_list)\n",
    "contributor_week_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_contributor_week_df = contributor_week_df[contributor_week_df.login.isin(core_team_usernames)==False]\n",
    "filtered_contributor_week_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df = pd.DataFrame({\"week_timestamp\" : list(all_weeks)})\n",
    "\n",
    "contributions = filtered_contributor_week_df.groupby(\"w\").sum()[\"c\"].to_frame().reset_index()\n",
    "contributions.columns = [\"week_timestamp\", \"total_contributions\"]\n",
    "week_df = week_df.merge(contributions, how='left', on=\"week_timestamp\")\n",
    "\n",
    "contributors = filtered_contributor_week_df.groupby(\"w\").sum()[\"contributors\"].to_frame().reset_index()\n",
    "contributors.columns = [\"week_timestamp\", \"total_contributors\"]\n",
    "week_df = week_df.merge(contributors, how='left', on=\"week_timestamp\")\n",
    "\n",
    "contributors = filtered_contributor_week_df.groupby(\"w\").sum()[\"first_contrib\"].to_frame().reset_index()\n",
    "contributors.columns = [\"week_timestamp\", \"new_contributors\"]\n",
    "week_df = week_df.merge(contributors, how='left', on=\"week_timestamp\")\n",
    "\n",
    "week_df = week_df.fillna(0)\n",
    "\n",
    "week_df[\"week_dates\"] = week_df.week_timestamp.map(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "\n",
    "week_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e6316c",
   "metadata": {},
   "outputs": [],
   "source": [
    "week_df.sort_values('week_timestamp', inplace=True)\n",
    "week_df.week_timestamp[:13]\n",
    "\n",
    "# plt.plot(week_df.week_timestamp[-13:], week_df.total_contributions[-13:])\n",
    "# plt.scatter(week_df.week_timestamp[-13:], week_df.total_contributions[-13:])\n",
    "# plt.xticks(week_df.week_timestamp[-13:], week_df.week_dates[-13:].map(lambda x: str(x)[:11]), rotation=45, ha=\"right\")\n",
    "# plt.title(\"Outside contributions (commits) per week\")\n",
    "\n",
    "# plt.savefig(\"figures/commits_per_week_1q_trailing_\"+str(datetime.date.today())+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c47edb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(13), week_df.total_contributors[-13:], width=0.45)\n",
    "plt.xticks(range(13), week_df.week_dates[-13:], rotation='vertical')\n",
    "\n",
    "plt.bar(range(13), week_df.new_contributors[-13:], width=0.45)\n",
    "plt.xticks(range(13), week_df.week_dates[-13:], rotation='vertical')\n",
    "plt.title('Contributors per week')\n",
    "\n",
    "plt.legend([\"Unique\", \"New\"])\n",
    "\n",
    "# plt.savefig(\"figures/contributors_per_week_1q_trailing_\"+str(datetime.date.today())+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e89d4",
   "metadata": {},
   "source": [
    "# PR Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a953a72a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_pr_request_url = \"https://api.github.com/repos/great-expectations/great_expectations/pulls?state=all&sort=created&direction=desc\"\n",
    "pr_infos = []\n",
    "# we are using GH API's paging\n",
    "while list_pr_request_url is not None :\n",
    "    pr_list_response = send_request_to_github_api(list_pr_request_url)\n",
    "\n",
    "    pr_infos.extend(pr_list_response.json())\n",
    "\n",
    "    # response.links has \"next\" {'url': '...&page=3', 'rel': 'next'}\n",
    "    next_page_info = pr_list_response.links.get(\"next\")\n",
    "    if next_page_info is None:\n",
    "        break\n",
    "    list_pr_request_url = next_page_info[\"url\"]\n",
    "\n",
    "    sorted_page_pr_create_datetimes = sorted([pr_info[\"created_at\"] for pr_info in pr_list_response.json()])\n",
    "    print(\"Paged response: {0:d} PRs, min created at: {1:s}\".format(len(sorted_page_pr_create_datetimes), sorted_page_pr_create_datetimes[0] if len(sorted_page_pr_create_datetimes) > 0 else \"\")) \n",
    "    if len(sorted_page_pr_create_datetimes) > 0 and sorted_page_pr_create_datetimes[0] < min_report_created_datetime_str:\n",
    "        break\n",
    "\n",
    "#eligible_pr_infos = [pr_info for pr_info in pr_infos if (pr_info[\"user\"][\"login\"] not in core_team_usernames and not pr_info[\"draft\"] and pr_info[\"created_at\"] >= min_pr_created_datetime )]\n",
    "eligible_pr_infos = [pr_info for pr_info in pr_infos if (pr_info[\"user\"][\"login\"] not in core_team_usernames and not pr_info[\"draft\"] and pr_info[\"created_at\"] >= str(min_report_created_datetime) )]\n",
    "print(\"Fetched {0:d} PRs. Out of them {1:d} eligible for the report.\".format(len(pr_infos), len(eligible_pr_infos)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81942e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_info_rows = [get_all_info_for_pr_row(pr_info) for pr_info in eligible_pr_infos]\n",
    "\n",
    "df_prs = pd.DataFrame(pr_info_rows)\n",
    "df_prs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1722d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prs[\"week\"] = df_prs.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"created_dt\"])), axis=1) \n",
    "df_prs[\"week_dt\"] = df_prs.apply(lambda x: sunday_of_the_week_date(dateutil.parser.parse(x[\"created_dt\"])), axis=1) \n",
    "df_prs = df_prs[(df_prs[\"close_dt\"].isnull()) | (df_prs[\"merge_dt\"].notnull())]\n",
    "\n",
    "df_prs[\"created\"] = df_prs[\"created_dt\"].notnull().astype(int)\n",
    "df_prs[\"first_core_team_reaction\"] = df_prs[\"first_core_team_reaction_dt\"].notnull().astype(int)\n",
    "df_prs[\"first_core_team_review\"] = df_prs[\"first_core_team_review_dt\"].notnull().astype(int)\n",
    "df_prs[\"first_core_team_approval\"] = df_prs[\"first_core_team_approval_dt\"].notnull().astype(int)\n",
    "df_prs[\"merge\"] = df_prs[\"merge_dt\"].notnull().astype(int)\n",
    "df_prs[\"close\"] = df_prs[\"close_dt\"].notnull().astype(int)\n",
    "df_prs\n",
    "\n",
    "df_report = df_prs.pivot_table(index=[\"week\"], values=[\"created\", \"first_core_team_reaction\", \"first_core_team_review\", \"first_core_team_approval\", \"merge\"], aggfunc=[np.sum])\n",
    "df_report = df_report.reset_index()\n",
    "df_report.columns = [\"week\", \"created\", \"approved\", \"reacted\", \"reviewed\", \"merged\"]\n",
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5db69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "_, ax = plt.subplots()\n",
    "# df_report[df_report[\"week\"]>\"2020-09-27\"].plot(kind= 'bar', x=\"week\", y=[\"created\", \"reacted\", \"reviewed\", \"approved\", \"merged\"])\n",
    "#df_report.plot(kind= 'bar', x=\"week\", y=[\"created\", \"reacted\", \"reviewed\", \"approved\", \"merged\"])\n",
    "plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "df_report.created.plot(kind='bar', ax=ax, color='red')\n",
    "df_report.reacted.plot(kind='bar', ax=ax, color='grey')\n",
    "df_report.reviewed.plot(kind='bar', ax=ax, color='blue')\n",
    "df_report.approved.plot(kind='bar', ax=ax, color='green')\n",
    "df_report.merged.plot(kind='bar', ax=ax, color='purple')\n",
    "ax.set_xticks(df_report.index)\n",
    "ax.set_xticklabels(df_report[\"week\"].values)\n",
    "cmap = plt.cm.coolwarm\n",
    "custom_lines = [Line2D([0], [0], color=\"red\", lw=10),\n",
    "                Line2D([0], [0], color=\"grey\", lw=10),\n",
    "                Line2D([0], [0], color=\"blue\", lw=10),\n",
    "                Line2D([0], [0], color=\"green\", lw=10),\n",
    "                Line2D([0], [0], color=\"purple\", lw=10),\n",
    "               ]\n",
    "ax.legend(custom_lines, ['No action taken', 'Acknlowledged', 'Reviewed', 'Approved', 'Merged'])\n",
    "plt.title(\"Community PRs - last 3 months, by weekly cohort\", fontsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_is_open_during_week(week_dt, x):\n",
    "    week_end_dt = week_dt+datetime.timedelta(days = 7)\n",
    "    created_dt = dateutil.parser.parse(x[\"created_dt\"])\n",
    "    close_dt = None\n",
    "    if not pd.isnull(x[\"close_dt\"]):\n",
    "        close_dt = dateutil.parser.parse(x[\"close_dt\"])\n",
    "    \n",
    "    return created_dt < week_end_dt and (pd.isnull(close_dt) or close_dt > week_end_dt)\n",
    "    \n",
    "\n",
    "# This is a not a Pandas way to do it, but gets the job done. Would be lovely to find out an elegant way to do this.    \n",
    "week_open_pr_counts = []    \n",
    "for week_dt_ref in df_prs[\"week_dt\"].unique():\n",
    "    s_tmp = df_prs.apply(lambda x: pr_is_open_during_week(week_dt_ref, x), axis=1)\n",
    "    week_open_pr_counts.append({\n",
    "        \"week_dt\": week_dt_ref,\n",
    "        \"count\": s_tmp[s_tmp == True].shape[0]\n",
    "    })\n",
    "\n",
    "df_num_open_community_prs_weekly = pd.DataFrame(week_open_pr_counts).sort_values(\"week_dt\")\n",
    "df_num_open_community_prs_weekly[\"week\"] = df_num_open_community_prs_weekly[\"week_dt\"].apply(lambda x: datetime.datetime.strftime(x, \"%Y-%m-%d\"))\n",
    "pd.DataFrame(week_open_pr_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d28183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "_, ax = plt.subplots()\n",
    "plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "df_num_open_community_prs_weekly.plot(kind='line', ax=ax, x=\"week\", y=\"count\", rot=90)\n",
    "ax.set_xticks(df_report.index)\n",
    "ax.set_xticklabels(df_num_open_community_prs_weekly[\"week\"].values)\n",
    "cmap = plt.cm.coolwarm\n",
    "plt.title('Number of open community PRs', fontsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d18286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the week a PR was created, merged, and closed if not merged\n",
    "pr_burn_date_df = pd.DataFrame(pr_info_rows)\n",
    "pr_burn_date_df = pr_burn_date_df[[\"created_dt\", \"merge_dt\", \"close_dt\"]]\n",
    "pr_burn_date_df[\"created_this_week\"] = pr_burn_date_df.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"created_dt\"])), axis=1)\n",
    "pr_burn_date_df[\"merged_this_week\"] = pr_burn_date_df.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"merge_dt\"]) if x[\"merge_dt\"] is not None else None), axis=1)\n",
    "pr_burn_date_df[\"closed_this_week\"] = pr_burn_date_df.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"close_dt\"]) if x[\"close_dt\"] is not None else None), axis=1)\n",
    "pr_burn_date_df = pr_burn_date_df[[\"created_this_week\", \"merged_this_week\", \"closed_this_week\"]]\n",
    "pr_burn_date_df[\"closed_no_merge_this_week\"] = np.where(pr_burn_date_df[\"merged_this_week\"].isnull() & pr_burn_date_df[\"closed_this_week\"].notnull(), pr_burn_date_df[\"closed_this_week\"], None)\n",
    "\n",
    "# count each category out\n",
    "all_date_values = pr_burn_date_df.values.ravel()\n",
    "unique_dates = pd.unique(all_date_values)\n",
    "unique_dates = np.sort(unique_dates[unique_dates != np.array(None)])\n",
    "pr_burn_df = pd.DataFrame(data=unique_dates, columns=[\"week\"])\n",
    "created_count = pr_burn_date_df[\"created_this_week\"].value_counts()\n",
    "merged_count = pr_burn_date_df[\"merged_this_week\"].value_counts()\n",
    "closed_no_merge_count = pr_burn_date_df[\"closed_no_merge_this_week\"].value_counts()\n",
    "pr_burn_df = pr_burn_df.merge(created_count, how=\"outer\", left_on=\"week\", right_index=True)\n",
    "pr_burn_df = pr_burn_df.merge(merged_count, how=\"outer\", left_on=\"week\", right_index=True)\n",
    "pr_burn_df = pr_burn_df.merge(closed_no_merge_count, how=\"outer\", left_on=\"week\", right_index=True)\n",
    "pr_burn_df = pr_burn_df.fillna(0)\n",
    "pr_burn_df[\"merged_or_closed_this_week\"] = pr_burn_df[\"merged_this_week\"] + pr_burn_df[\"closed_no_merge_this_week\"]\n",
    "\n",
    "# calculate cumulative open PRs\n",
    "open_prs_start_of_first_week = 21\n",
    "pr_burn_df[\"change_in_open_prs\"] = pr_burn_df[\"created_this_week\"] - pr_burn_df[\"merged_or_closed_this_week\"]\n",
    "pr_burn_df[\"cumulative_open_prs\"] = pr_burn_df[\"change_in_open_prs\"].cumsum()\n",
    "pr_burn_df[\"cumulative_open_prs\"] = pr_burn_df[\"cumulative_open_prs\"] + open_prs_start_of_first_week\n",
    "pr_burn_df = pr_burn_df.iloc[-13:]\n",
    "pr_burn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb338209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe just for plot\n",
    "pr_plot_df = pr_burn_df[[\"week\", \"created_this_week\", \"merged_or_closed_this_week\", \"change_in_open_prs\", \"cumulative_open_prs\"]]\n",
    "pr_plot_df[\"merged_or_closed_this_week\"] = pr_plot_df[\"merged_or_closed_this_week\"] * -1\n",
    "column_names = [\"Week\", \"Created this Week\", \"Merged or Closed this Week\", \"Change in Open PRs\", \"Cumulative Open PRs\"]\n",
    "pr_plot_df.columns = column_names\n",
    "\n",
    "# plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": [4, 1]})\n",
    "\n",
    "pr_plot_df.plot(x=\"Week\", y=\"Cumulative Open PRs\", kind=\"line\", ax=ax1, drawstyle='steps-mid', color=\"black\")\n",
    "pr_plot_df.plot(x=\"Week\", y=\"Created this Week\", kind=\"bar\", ax=ax1, color=\"salmon\", bottom=pr_plot_df[\"Cumulative Open PRs\"])\n",
    "pr_plot_df.plot(x=\"Week\", y=\"Merged or Closed this Week\", kind=\"bar\", ax=ax1, color=\"lightgreen\", bottom=pr_plot_df[\"Cumulative Open PRs\"])\n",
    "pr_plot_df[\"min\"] = pr_plot_df[\"Cumulative Open PRs\"] + pr_plot_df[\"Merged or Closed this Week\"]\n",
    "pr_plot_df[\"max\"] = pr_plot_df[\"Cumulative Open PRs\"] + pr_plot_df[\"Created this Week\"]\n",
    "ax1.set_ylim([pr_plot_df[\"min\"].min()-3, pr_plot_df[\"max\"].max()+6])\n",
    "pr_plot_df = pr_plot_df.drop(axis=1, columns=[\"max\", \"min\"])\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "\n",
    "change_colors = np.where(pr_plot_df[\"Change in Open PRs\"] <= 0, \"lightgreen\", \"salmon\")\n",
    "markers = np.where(pr_plot_df[\"Change in Open PRs\"] <= 0, \"v\", \"^\")\n",
    "\n",
    "pr_plot_df[\"dummy\"] = [5] * len(pr_plot_df)\n",
    "for i in range(len(pr_plot_df)):\n",
    "    ax2.set_ylim([-5,10])\n",
    "    ax2.scatter(x=pr_plot_df[\"Week\"].iloc[i], y=pr_plot_df[\"dummy\"].iloc[i], marker=markers[i], color=change_colors[i], s=500, edgecolors=\"black\")\n",
    "    ax2.annotate(text=pr_plot_df[\"Change in Open PRs\"].iloc[i].astype(int), xy=(pr_plot_df[\"Week\"].iloc[i], -2), ha=\"center\", fontsize=20)\n",
    "pr_plot_df = pr_plot_df.drop(axis=1, columns=[\"dummy\"])\n",
    "ax2.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.grid(b=True, axis=\"y\")\n",
    "ax1.tick_params(axis=\"y\", which=\"major\", labelsize=16)\n",
    "ax1.legend(loc=2, fontsize=16)\n",
    "ax1.set_title('Cumulative Change in Open Community PRs (Previous 13 Weeks)', fontdict={'fontsize': 25})\n",
    "ax2.tick_params(axis=\"x\", which=\"major\", labelsize=16, rotation=60)\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99887df",
   "metadata": {},
   "source": [
    "# Issue Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7bbc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_issues_request_url = \"https://api.github.com/repos/great-expectations/great_expectations/issues?state=all&sort=created&direction=desc\"\n",
    "issue_infos = []\n",
    "# we are using GH API's paging\n",
    "while list_issues_request_url is not None :\n",
    "    issue_list_response = send_request_to_github_api(list_issues_request_url)\n",
    "\n",
    "    issue_infos.extend(issue_list_response.json())\n",
    "\n",
    "    # response.links has \"next\" {'url': '...&page=3', 'rel': 'next'}\n",
    "    next_page_info = issue_list_response.links.get(\"next\")\n",
    "    if next_page_info is None:\n",
    "        break\n",
    "    list_issues_request_url = next_page_info[\"url\"]\n",
    "\n",
    "    sorted_page_pr_create_datetimes = sorted([pr_info[\"created_at\"] for pr_info in issue_list_response.json()])\n",
    "    print(\"Paged response: {0:d} issues, min created at: {1:s}\".format(len(sorted_page_pr_create_datetimes), sorted_page_pr_create_datetimes[0] if len(sorted_page_pr_create_datetimes) > 0 else \"\")) \n",
    "    if len(sorted_page_pr_create_datetimes) > 0 and sorted_page_pr_create_datetimes[0] < min_fetch_created_datetime:\n",
    "        break\n",
    "\n",
    "eligible_issue_infos = [issue_info for issue_info in issue_infos if (not issue_info.get(\"pull_request\") and issue_info[\"created_at\"] >= min_fetch_created_datetime )]\n",
    "print(\"Fetched {0:d} issues. Out of them {1:d} eligible for the report.\".format(len(issue_infos), len(eligible_issue_infos)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a492a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_info_rows = [get_all_info_for_issue_row(issue_info) for issue_info in eligible_issue_infos]\n",
    "df_issues = pd.DataFrame(issue_info_rows)\n",
    "df_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af32c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"week\"] = df_issues.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"created_dt\"])), axis=1) \n",
    "df_issues[\"week_dt\"] = df_issues.apply(lambda x: sunday_of_the_week_date(dateutil.parser.parse(x[\"created_dt\"])), axis=1) \n",
    "df_issues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c763c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"created\"] = df_issues[\"created_dt\"].notnull().astype(int)\n",
    "df_issues[\"first_core_team_comment\"] = (df_issues[\"first_core_team_comment_dt\"].notnull() | df_issues[\"close_dt\"].notnull()).astype(int)\n",
    "df_issues[\"close\"] = df_issues[\"close_dt\"].notnull().astype(int)\n",
    "\n",
    "df_community_reported_issues = df_issues[df_issues.apply(lambda x: x[\"author_login\"] not in core_team_usernames, axis=1)] \n",
    "print(\"all issues\", df_issues.shape)\n",
    "print(\"community issues\", df_community_reported_issues.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7923208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = df_community_reported_issues[df_community_reported_issues[\"created_dt\"] > min_report_created_datetime_str].pivot_table(index=[\"week\"], values=[\"created\", \"first_core_team_comment\", \"close\"], aggfunc=[np.sum])\n",
    "df_report = df_report.reset_index()\n",
    "df_report.columns = [\"week\", \"closed\", \"created\", \"responded\"]\n",
    "df_report.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f391a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "#plt.rcParams.update({'axes.titlesize': 'small'})\n",
    "plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "_, ax = plt.subplots()\n",
    "plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "df_report.created.plot(kind='bar', ax=ax, color='red')\n",
    "df_report.responded.plot(kind='bar', ax=ax, color='grey')\n",
    "df_report.closed.plot(kind='bar', ax=ax, color='purple')\n",
    "ax.set_xticks(df_report.index)\n",
    "ax.set_xticklabels(df_report[\"week\"].values)\n",
    "cmap = plt.cm.coolwarm\n",
    "custom_lines = [Line2D([0], [0], color=\"red\", lw=10),\n",
    "                Line2D([0], [0], color=\"grey\", lw=10),\n",
    "                Line2D([0], [0], color=\"purple\", lw=10)]\n",
    "ax.legend(custom_lines, ['No action taken', 'Acknlowledged', 'Closed'])\n",
    "plt.title('Weekly cohorts of community-reported GH issues', fontsize=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33397a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_issues[\"week_dt\"].unique()\n",
    "def issue_is_open_during_week(week_dt, x, column_tag: str):\n",
    "    week_end_dt = week_dt+datetime.timedelta(days = 7)\n",
    "    created_dt = dateutil.parser.parse(x[\"created_dt\"])\n",
    "    close_dt = None\n",
    "    if not pd.isnull(x[\"close_dt\"]):\n",
    "        close_dt = dateutil.parser.parse(x[\"close_dt\"])\n",
    "    \n",
    "    return x[column_tag] and created_dt < week_end_dt and (pd.isnull(close_dt) or close_dt > week_end_dt)\n",
    "    \n",
    "\n",
    "# This is a not a Pandas way to do it, but gets the job done. Would be lovely to find out an elegant way to do this.\n",
    "week_open_issue_counts = []    \n",
    "for week_dt_ref in df_issues[df_issues[\"created_dt\"] > min_report_created_datetime_str][\"week_dt\"].unique():\n",
    "    devrel_count = df_issues.apply(lambda x: issue_is_open_during_week(week_dt_ref, x, \"is_devrel\"), axis=1)\n",
    "    core_engineering_queue_count = df_issues.apply(lambda x: issue_is_open_during_week(week_dt_ref, x, \"is_core_engineering_queue\"), axis=1)\n",
    "    help_wanted_count = df_issues.apply(lambda x: issue_is_open_during_week(week_dt_ref, x, \"is_help_wanted\"), axis=1)\n",
    "    triage_count = df_issues.apply(lambda x: issue_is_open_during_week(week_dt_ref, x, \"is_triage\"), axis=1)\n",
    "    \n",
    "    week_open_issue_counts.append({\n",
    "        \"week_dt\": week_dt_ref,\n",
    "        \"devrel_count\": devrel_count[devrel_count == True].shape[0],\n",
    "        \"core_engineering_queue_count\": core_engineering_queue_count[core_engineering_queue_count == True].shape[0],\n",
    "        \"help_wanted_count\": help_wanted_count[help_wanted_count == True].shape[0],\n",
    "        \"triage_count\": triage_count[triage_count == True].shape[0], \n",
    "    })\n",
    "\n",
    "df_num_open_core_team_priority_issues_weekly = pd.DataFrame(week_open_issue_counts).sort_values(\"week_dt\")\n",
    "df_num_open_core_team_priority_issues_weekly[\"week\"] = df_num_open_core_team_priority_issues_weekly[\"week_dt\"].apply(lambda x: datetime.datetime.strftime(x, \"%Y-%m-%d\"))\n",
    "df_num_open_core_team_priority_issues_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfeb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "plt.rcParams[\"figure.figsize\"] = (20,7)\n",
    "_, ax = plt.subplots()\n",
    "plt.legend(bbox_to_anchor=(1.0, 1), loc='upper left')\n",
    "\n",
    "for label in [\n",
    "    \"devrel_count\",\n",
    "    \"core_engineering_queue_count\",\n",
    "    \"help_wanted_count\",\n",
    "    \"triage_count\"\n",
    "]:\n",
    "    df_num_open_core_team_priority_issues_weekly.plot(kind='line', ax=ax, x=\"week\", y=label, rot=90)\n",
    "\n",
    "ax.set_xticks(df_report.index)\n",
    "ax.set_xticklabels(df_num_open_core_team_priority_issues_weekly[\"week\"].values)\n",
    "cmap = plt.cm.coolwarm\n",
    "plt.title('Number of open community issues', fontsize=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the week an issue was created, merged, and closed if not merged\n",
    "issue_burn_date_df = pd.DataFrame(issue_info_rows)\n",
    "issue_burn_date_df = issue_burn_date_df[[\"created_dt\", \"close_dt\"]]\n",
    "issue_burn_date_df[\"created_this_week\"] = issue_burn_date_df.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"created_dt\"])), axis=1)\n",
    "issue_burn_date_df[\"closed_this_week\"] = issue_burn_date_df.apply(lambda x: sunday_of_the_week_date_str(dateutil.parser.parse(x[\"close_dt\"]) if x[\"close_dt\"] is not None else None), axis=1)\n",
    "issue_burn_date_df = issue_burn_date_df[[\"created_this_week\", \"closed_this_week\"]]\n",
    "\n",
    "# count each category out\n",
    "all_date_values = issue_burn_date_df.values.ravel()\n",
    "unique_dates = pd.unique(all_date_values)\n",
    "unique_dates = np.sort(unique_dates[unique_dates != np.array(None)])\n",
    "issue_burn_df = pd.DataFrame(data=unique_dates, columns=[\"week\"])\n",
    "created_count = issue_burn_date_df[\"created_this_week\"].value_counts()\n",
    "closed_count = issue_burn_date_df[\"closed_this_week\"].value_counts()\n",
    "issue_burn_df = issue_burn_df.merge(created_count, how=\"outer\", left_on=\"week\", right_index=True)\n",
    "issue_burn_df = issue_burn_df.merge(closed_count, how=\"outer\", left_on=\"week\", right_index=True)\n",
    "issue_burn_df = issue_burn_df.fillna(0)\n",
    "\n",
    "# calculate cumulative open issues\n",
    "open_issues_start_of_first_week = 1\n",
    "issue_burn_df[\"change_in_open_issues\"] = issue_burn_df[\"created_this_week\"] - issue_burn_df[\"closed_this_week\"]\n",
    "issue_burn_df[\"cumulative_open_issues\"] = issue_burn_df[\"change_in_open_issues\"].cumsum()\n",
    "issue_burn_df[\"cumulative_open_issues\"] = issue_burn_df[\"cumulative_open_issues\"] + open_issues_start_of_first_week\n",
    "issue_burn_df = issue_burn_df.iloc[-13:]\n",
    "issue_burn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc143576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe just for plot\n",
    "issue_plot_df = issue_burn_df[[\"week\", \"created_this_week\", \"closed_this_week\", \"change_in_open_issues\", \"cumulative_open_issues\"]]\n",
    "issue_plot_df[\"closed_this_week\"] = issue_plot_df[\"closed_this_week\"] * -1\n",
    "column_names = [\"Week\", \"Created this Week\", \"Closed this Week\", \"Change in Open Issues\", \"Cumulative Open Issues\"]\n",
    "issue_plot_df.columns = column_names\n",
    "\n",
    "# plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": [4, 1]})\n",
    "\n",
    "issue_plot_df.plot(x=\"Week\", y=\"Cumulative Open Issues\", kind=\"line\", ax=ax1, drawstyle='steps-mid', color=\"black\")\n",
    "issue_plot_df.plot(x=\"Week\", y=\"Created this Week\", kind=\"bar\", ax=ax1, color=\"salmon\", bottom=issue_plot_df[\"Cumulative Open Issues\"])\n",
    "issue_plot_df.plot(x=\"Week\", y=\"Closed this Week\", kind=\"bar\", ax=ax1, color=\"lightgreen\", bottom=issue_plot_df[\"Cumulative Open Issues\"])\n",
    "issue_plot_df[\"min\"] = issue_plot_df[\"Cumulative Open Issues\"] + issue_plot_df[\"Closed this Week\"]\n",
    "issue_plot_df[\"max\"] = issue_plot_df[\"Cumulative Open Issues\"] + issue_plot_df[\"Created this Week\"]\n",
    "ax1.set_ylim([issue_plot_df[\"min\"].min()-3, issue_plot_df[\"max\"].max()+6])\n",
    "issue_plot_df = issue_plot_df.drop(axis=1, columns=[\"max\", \"min\"])\n",
    "ax1.get_xaxis().set_visible(False)\n",
    "\n",
    "change_colors = np.where(issue_plot_df[\"Change in Open Issues\"] <= 0, \"lightgreen\", \"salmon\")\n",
    "markers = np.where(issue_plot_df[\"Change in Open Issues\"] <= 0, \"v\", \"^\")\n",
    "\n",
    "issue_plot_df[\"dummy\"] = [5] * len(issue_plot_df)\n",
    "for i in range(len(issue_plot_df)):\n",
    "    ax2.set_ylim([-5,10])\n",
    "    ax2.scatter(x=issue_plot_df[\"Week\"].iloc[i], y=issue_plot_df[\"dummy\"].iloc[i], marker=markers[i], color=change_colors[i], s=500, edgecolors=\"black\")\n",
    "    ax2.annotate(text=issue_plot_df[\"Change in Open Issues\"].iloc[i].astype(int), xy=(issue_plot_df[\"Week\"].iloc[i], -2), ha=\"center\", fontsize=20)\n",
    "issue_plot_df = issue_plot_df.drop(axis=1, columns=[\"dummy\"])\n",
    "ax2.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout()\n",
    "ax1.grid(b=True, axis=\"y\")\n",
    "ax1.tick_params(axis=\"y\", which=\"major\", labelsize=16)\n",
    "ax1.legend(loc=2, fontsize=16)\n",
    "ax1.set_title('Cumulative Change in Open Community Issues (Previous 13 Weeks)', fontdict={'fontsize': 25})\n",
    "ax2.tick_params(axis=\"x\", which=\"major\", labelsize=16, rotation=60)\n",
    "plt.subplots_adjust(hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recent_issue_priority_categories = pd.DataFrame(df_issues[df_issues[\"week\"] > min_report_created_datetime_str].groupby(\"priority_category\")[\"number\"].count())#.reset_index()\n",
    "df_recent_issue_priority_categories.columns = [\"count\"]\n",
    "df_recent_issue_priority_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fb82fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_recent_issue_priority_categories.plot(kind=\"pie\",  y=\"count\", autopct='%.1f')\n",
    "plt.title('Issue Prioritization (reported by anyone)', fontsize=25)\n",
    "\n",
    "ax.get_legend().remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeee06f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "251px",
    "width": "291px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
